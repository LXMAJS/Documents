## 第三章、复杂度分析

### 一、为什么要分析、同计算法的执行效率和资源消耗？

在代码执行期间，可以通过统计、监控等办法得到算法的执行时间和占用的内存大小，这种办法可以叫做事后统计法，但这种办法有很多局限性：

1. 测试结果非常依赖测试环境
   测试环境中的硬件不同，会对测试的结果有很大的影响，比如我们拿一段相同的代码，分别在 Intel Core i9 处理器和 Intel Core i3 处理器上来执行，不用说 i9 处理器要比 i3 处理器的执行速度快很多。还有，比如原本在和台机器上 a 代码执行的速度比 b 代码要快，等我们换到另一台机器上时，可能会有截然相反的结果。
2. 测试结果受数据规模的影响很大
   对于同一个排序算法，待排序的数据有序度不一样，执行时间就有很大的差别，极端情况下，如果数据已经是有序的，那排序算法不需要做人呢和操作，时间就非常短。除此之外，如果测试数据的国模太小，测试结果可能无法真实地反应算法的性能，比如，对于小规模的排序，插入排序可能反而比快排要快。

所以我们应该在不考虑具体数据的情况下，就能粗略估计算法的执行效率，这就是时间、空间复杂度分析办法。

### 二、大O复杂度表示法

以下面的代码为例，求 1,2,3 ... n 的累加和：

``` java
int cal(int n) {
  int sum = 0;
  int i = 1;
  for (; i <= n; ++i) {
    sum = sum + i;
  }
  return sum;
}
```

从 CPU 的角度来看，这段代码的每一行都执行类似的操作：读数据、运算、写数据。假设每行代码执行的时间都一样，为 unit_time（单位时间），第 2、3 行代码分别需要1个 unit_time，第 4、5 行都运行了 n 遍，所以需要 2n * unit_time 的执行时间，所以这个函数所需要的执行时间为：``(2n+2) * unit_time``。

按照这个思路，来分析下面的代码：

```java
int cal(int n) {
  int sum = 0;
  int i = 1;
  int j = 1;
  for(; i <= n; ++i) {
    j = 1;
    for(; j <= n; ++j) {
      sum = sum + i * j;
    }
  }
  return sum;
}
```

这是一个求 n*n 的二维数组和的方法，假设每个语句执行的时间是 unit_time，第 2、3、4 分别执行 1次，5、6分别执行 n次，7、8分别执行 n² 次，所以整段代码需要执行 ``(2n² + 2n + 3) * unit_time``。

由此，我们可以总结一个公示：**T(n) = O(f(n))**

T(n) 表示执行的时间，n 表示数据规模的大小，f(n) 表示每行代码执行的次数总和，O表示代码的执行时间 T(n) 和 f(n) 成正比。所以，第一个例子中的 T(n) = O(2n+2)，第二个例子中的 T(n) = O(2n²+2n+3)。这就是**大O时间复杂度表示法**，它并不具体的表示代码真正的执行时间，二表示代码执行时间随数据规模增长的变化趋势，所以也读作：渐进时间复杂度，简称 时间复杂度。

当 n 很大时，公式中的低阶、常量 和 系数 并不左右增长的趋势，所以可以忽略，我们只需要记录一个最大量级即可，所以上面的例子中，通常将大O表示法写作：T(n) = O(n)，T(n) = O(n²)。

### 三、时间复杂度

分析时间复杂度，可以从一下几个方面入手。

#### 1、只关注循环次数最多的一段代码

因为大O表示法只表示一种变化趋势，因此我们在分析代码时，也可以只关注循环次数最多的一段代码，因为这段代码执行次数的 n 的量级，就是整段代码要分析的时间复杂度。用上面的第一个例子来说明：

``` java
int cal(int n) {
  int sum = 0;
  int i = 1;
  for (; i <= n; ++i) {
    sum = sum + i;
  }
  return sum;
}
```

其中，第 2、3 行都是常量级的执行时间，与 n 无关，循环次数最多的是第 4、5 航带吗，所以这里需要重点分析。因为它们分别需要执行 n 次，因此整个代码的时间复杂度也就是 O(n)。

#### 2、加法法则：总复杂度等于量级最大的那段代码的复杂度

```java
int cal(int n) {
  int sum_1 = 0;
  int p = 1;
  for (; p <= 100; ++p) {
    sum_1 = sum_1 + p;
  }
  
  int sum_2 = 0;
  int i = 1;
  int j = 1;
  for(; i <= n; ++i) {
    j = 1;
    for(; j <= n; ++j) {
      sum_2 = sum_2 + i * j;
    }
  }
  return sum_1 + sum_2;
}
```

这段代码分为2部分，其中第一部分是计算 100以内数字的总和，第二部分是计算 n阶二维数组的总和。第一部分的代码中，for 循环执行了 100次，第二部分的代码中，内部的 for循环执行了 n²次，根据我们的方法，常量的执行时间与 n 无关，因此他的时间复杂度是 T(n) = O(n²)。

#### 3、乘法法则：嵌套代码的复杂度，等于嵌套内外代码复杂度的乘积

如果 T(n) = O( f(n) )，T(2n) = O( g(n) )，那么 T(n) = T1(n) * T2(n) = O( f(n) ) * O( g(n) ) = O( f(n) * g(n) )。 

### 四、集中常见的时间复杂度实例分析

##### 1、O(1)

O(1) 是常量级时间复杂度的表示方法，并不是只执行了一行代码。一般来说，只要算法中不存在循环、递归，起时间复杂度就是 O(1)。

##### 2、O(logn)、O(nlogn)

对数阶时间复杂度十分场见，通知也是很难分析的一种时间复杂度。例如：

``` java
int i = 1;
while( i < n ) {
  i = i * 3;
}
```

根据刚刚的思路，很简单能看出，这段代码的时间复杂度是 $O(log_3n)$，因为对数质检可以相互转换，所以不管对数以2为底，或者以3为底，他的时间复杂度都是 O(logn)。如果上面的代码中执行了n遍，那么他的时间复杂度就是 O(nlogn)。

##### 3、O(m+n)、O(m*n)

假设代码中出现了2个变量，而这两个变量的数据规模是无法对比，或无法估量的，那么在表示复杂度的时候就不能简单的用假发原则，省略掉其中任何一个。比如下面的代码：

``` java
int cal(int m, int n) {
  int sum_1 = 0;
  for (int i = 0; i < m; ++i) {
    sum_1 = sum_1 + i;
  }
  int sum_2 = 0;
  for (int j = 1; j < n; ++j) {
    sum_2 = sum_2 + j;
  }
  return sum_1 + sum_2;
}
```

从代码上看分为2部分，第一部分复杂度为 O(m)，第二部分复杂度为 O(n)，那么整段代码的复杂度就是二者之和 O(m+n)。

### 五、空间复杂度分析

时间复杂度全称是 渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度 全称就是 渐进空间复杂度（Asynmptotic Space Complexity），表示算法的存储空间与数据规模之间的增长关系。以下面的代码为例：

``` java
void print(int n) {
  int[] a = new int[n];
  for (int i = 1; i < n; ++i) {
    a[i] = i * i;
  }
  for (i = n-1; i >= 0; --i) {
    System.out.println(a[i]);
  }
}
```

从代码中可以看到，for循环中申请了1个空间存储变量i，但它是常量阶的，跟n没有关系。而第2行申请了一个大小为 n 的数组，剩下的代码没有占用更多的空间，因此整段代码的空间复杂度就是 O(n)。

空间复杂度的分析相比时间复杂度来说简单很多，常见的都是 O(1)、O(n)、O(n²)，对数阶的复杂度平时都用不到。

### 六、最好、最坏情况时间复杂度

以下面的代码为例，它实现了一个方法：在一个无序数组 array 中，找出变量 x 出现的位置，如果没有找到就返回 -1：

``` java
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for(; i < n; ++1) {
    if(attay[i] == x) pos = i;
  }
  return pos;
}
```

从上面的代码上看，他的时间复杂度是 O(n)，但其实我们可以优化一下，当招到 x 的位置时，就可以退出循环了，优化后的代码如下：

``` java
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for(; i < n; ++1) {
    if(attay[i] == x) {
      pos = i;
      break;
    }
  }
  return pos;
}
```

这时候，我们知道这段代码的时间复杂度并不再是 O(n) 了，因为假设当 x 是数组的第一个元素时，其时间复杂度就是 O(1)，如果是最后一个元素，那么就是 O(n)。因此我们引入3个概念：**最好情况时间复杂度**、**最坏情况时间复杂度** 和 **平均情况时间复杂度**。

其中，前二者都比较好理解，可以从最理想的情况和最坏的情况来估算代码的时间复杂度。

### 七、平均情况时间复杂度

最好情况 和 最坏情况 在实际场景中出现的概率并不大，他们都是极端情况。为了更好的表示平均情况下的复杂度，我们引入了平均情况时间复杂度，简称 平均时间复杂度。以上面的例子，要查找变量 x 在数组中的位置，有 n+1 中情况：在 数组的 0 ~ n-1 的任意位置中，或者不在数组中。我们把每种情况要遍历的元素个数进行累加，再除以 n+1，就得到平均元素的个数，即：

// 公式

我们简化常量和阶数后，得到的平均时间复杂度是 O(n)。这个结论虽然正确，但计算过程有点问题，因为并不是每种情况出现的概率都是一样的，我们要从概率论的维度来分析。假设 x 在数组中 和 不在数组中 的概率各为 1/2，如果 x 在数组中，则可能出现在任意位置的概率就是 1/n，根据乘法法则可以知道 x 在数组中任意位置的概率是 1/2n。引入概率后我们就能得到下面的计算公式：

//  公式

得到的这个值就是概率论中的 加权平均值，也叫做期望。所以平均时间复杂度的全称应该叫做 **加权平均时间复杂度** 或 **期望时间复杂度**。

但是 O(3n+1 / 4) 简化后的表示也是 O(n)，这个结果其实与上面的结论差别不大，所以很多时候我们只需要使用一个复杂度就能满足分析需求了，只有在量级上有十分巨大的差别时，才会使用3种复杂度来分别区分。

### 八、均摊时间复杂度

对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们可以江浙一簇操作放在一起分析，看是否能将较高的时间复杂度的那次操作耗时，平摊到其他的操作上，这就是均摊时间复杂度分析法。

尽管很多数据结构和算法书籍都花了很大力气来区分 平均时间复杂度 和 均摊时间复杂度，但我个人认为：**均摊时间复杂度就是一种特殊的平均时间复杂度**。我们没必要花时间来区分他们。
